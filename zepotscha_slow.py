"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Ü–µ–ø–æ—á–µ–∫ —Å–ª–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –≥–ª—É–±–æ–∫–∏–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import json
import random
import os
import time
import re
from typing import List, Dict, Optional, Tuple, Set
from collections import defaultdict


class AdvancedWordChainGenerator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ü–µ–ø–æ—á–µ–∫ —Å –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""

    def __init__(self, db_file: str = "advanced_word_chain_db.json"):
        self.db_file = db_file
        self.db = self._load_database()
        random.seed(time.time() + os.getpid())

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤
        self.context_patterns = {
            "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": [
                r"(–∫—Ç–æ|—á—Ç–æ) (—Ç–∞–∫–æ–µ|—Ç–∞–∫–æ–π|—Ç–∞–∫–∞—è|—Ç–∞–∫–∏–µ|—Ç–∞–∫–∏–º–∏)",
                r"–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ\s+",
                r"—á—Ç–æ –∑–Ω–∞—á–∏—Ç\s+",
                r"–∫—Ç–æ —Ç–∞–∫–æ–π\s+",
                r"–∫—Ç–æ —Ç–∞–∫–∞—è\s+",
                r"–æ–±—ä—è—Å–Ω–∏\s+",
                r"—Ä–∞—Å—Å–∫–∞–∂–∏\s+–æ\s+",
                r"—á—Ç–æ —Ç–∞–∫–æ–µ\s+"
            ],
            "–¥–µ–π—Å—Ç–≤–∏–µ": [
                r"—á—Ç–æ (–¥–µ–ª–∞–µ—Ç|–¥–µ–ª–∞—é—Ç|—É–º–µ–µ—Ç|—É–º–µ—é—Ç|–≥–æ–≤–æ—Ä–∏—Ç|–≥–æ–≤–æ—Ä—è—Ç)\s+",
                r"–∫–∞–∫ (–¥–µ–π—Å—Ç–≤—É–µ—Ç|—Ä–∞–±–æ—Ç–∞–µ—Ç|—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç)\s+",
                r"–∫–∞–∫–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ\s+",
                r"–∫–∞–∫ –≤–µ–¥–µ—Ç —Å–µ–±—è\s+"
            ],
            "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ": [
                r"—Å—Ä–∞–≤–Ω–∏\s+",
                r"—á–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\s+",
                r"–≤ —á–µ–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É\s+",
                r"–ø–æ—Ö–æ–∂ –Ω–∞\s+",
                r"–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç\s+"
            ],
            "–ø—Ä–∏–º–µ—Ä—ã": [
                r"–ø—Ä–∏–º–µ—Ä—ã?\s+",
                r"–ø—Ä–∏–≤–µ–¥–∏ –ø—Ä–∏–º–µ—Ä—ã?\s+",
                r"–¥–ª—è —á–µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\s+",
                r"–≥–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è\s+",
                r"–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è\s+"
            ],
            "—Å–≤–æ–π—Å—Ç–≤–∞": [
                r"–∫–∞–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞\s+",
                r"—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\s+",
                r"–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\s+",
                r"—á–µ—Ä—Ç—ã\s+",
                r"–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\s+"
                r"–∫–∞–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\s+"
            ]
        }

        # –°–∏–Ω–æ–Ω–∏–º—ã –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
        self.synonyms = {
            "–∂–∞–±–∞": ["–∑–µ–º–Ω–æ–≤–æ–¥–Ω–æ–µ", "–∞–º—Ñ–∏–±–∏—è", "–ª—è–≥—É—à–∫–∞", "–∫–≤–∞–∫—É—à–∫–∞"],
            "–ø–∞–π—Ç–æ–Ω": ["python", "–ø–∏—Ç–æ–Ω", "—è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"],
            "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ": ["–∫–æ–¥–∏–Ω–≥", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "–Ω–∞–ø–∏—Å–∞–Ω–∏–µ –∫–æ–¥–∞"],
            "–≥–∞–≤—Ä–∏–ª–æ–≤–Ω–∞": ["–≥–∞–≤—Ä–∏–ª–æ–≤–Ω—ã", "–≥–∞–≤—Ä–∏–ª–æ–≤–Ω—É", "–≥–∞–≤—Ä–∏–ª–æ–≤–Ω–µ"],
            "–±—É–±–ª–∏–∫": ["–±—É–±–ª–∏–∫–∞", "–±"]

        }

        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        self.base_knowledge = {
            "–∂–∞–±–∞": {
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "–ñ–∞–±–∞ ‚Äî —ç—Ç–æ –∑–µ–º–Ω–æ–≤–æ–¥–Ω–æ–µ –∂–∏–≤–æ—Ç–Ω–æ–µ –∏–∑ –æ—Ç—Ä—è–¥–∞ –±–µ—Å—Ö–≤–æ—Å—Ç—ã—Ö.",
                "–¥–µ–π—Å—Ç–≤–∏–µ": "–ñ–∞–±–∞ –ø—Ä—ã–≥–∞–µ—Ç, –ø–ª–∞–≤–∞–µ—Ç –∏ –ª–æ–≤–∏—Ç –Ω–∞—Å–µ–∫–æ–º—ã—Ö –¥–ª–∏–Ω–Ω—ã–º —è–∑—ã–∫–æ–º.",
                "–∑–≤—É–∫–∏": "–ñ–∞–±–∞ –∏–∑–¥–∞–µ—Ç –∑–≤—É–∫–∏ '–∫–≤–∞-–∫–≤–∞' –∏–ª–∏ '—É—Ä—Ä-—É—Ä—Ä'.",
                "—Å—Ä–µ–¥–∞": "–ñ–∏–≤–µ—Ç –≤–±–ª–∏–∑–∏ –≤–æ–¥–æ–µ–º–æ–≤, –≤–æ –≤–ª–∞–∂–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö.",
                "–ø–∏—Ç–∞–Ω–∏–µ": "–ü–∏—Ç–∞–µ—Ç—Å—è –Ω–∞—Å–µ–∫–æ–º—ã–º–∏, —á–µ—Ä–≤—è–º–∏, –º–µ–ª–∫–∏–º–∏ –±–µ—Å–ø–æ–∑–≤–æ–Ω–æ—á–Ω—ã–º–∏."
            },
            "–ø–∞–π—Ç–æ–Ω": {
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "Python ‚Äî —ç—Ç–æ –≤—ã—Å–æ–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ–±—â–µ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.",
                "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏, –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.",
                "—Å–∏–Ω—Ç–∞–∫—Å–∏—Å": "–ò–º–µ–µ—Ç –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –µ–≥–æ —Ö–æ—Ä–æ—à–∏–º –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤.",
                "–æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º—ã–π —è–∑—ã–∫ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π.",
                "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": "–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –≤ Google, YouTube, Instagram, NASA."
            },
            "–≥–∞–≤—Ä–∏–ª–æ–≤–Ω–∞": {
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "–ì–∞–≤—Ä–∏–ª–æ–≤–Ω–∞ ‚Äî –≠—Ç–æ –≥–ª–∞–≤–Ω–∞—è —É–≥—Ä–æ–∑–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–π —Ñ–µ–¥–µ—Ä–∞—Ü–∏–∏",
                "–∫–æ–Ω—Ç–µ–∫—Å—Ç": "–ì–∞–≤—Ä–∏–ª–æ–≤–Ω–∞ —ç—Ç–æ —É–≥—Ä–æ–∑–∞ —Å—É—â–µ—Å—Ç–æ–≤–∞–Ω–∏—é —á–µ–ª–æ–≤–µ—á–∏—Å—Ç–≤–∞",
                "–ø—Ä–∏–º–µ—Ä": "–ì–∞–≤—Ä–∏–ª–æ–≤–Ω–∞ —Å–∏–ª–æ–π –º—ã—Å–ª–∏ —Ä–∞–∑–æ—Ä–≤–∞–ª–∞ –º–µ–∂ –º–∞–ª–µ–∫—É–ª—è—Ä–Ω—É—é —Å–≤—è–∑—å"
            },
            "–∫—É—Ä—Å –±—É–±–ª–∏–∫–∞": {
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": "–ö—É—Ä—Å –ë—É–±–ª–∏–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –æ—Ç 8 —Ñ–µ–≤—Ä–∞–ª—è 2025 –≥–æ–¥–∞ –∏ —Å–µ–π—á–∞—Å —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 0.031337‚ÇΩ",
                "–∑–Ω–∞—á–µ–Ω–∏–µ": "–ö—É—Ä—Å –ë—É–±–ª–∏–∫–∞ –Ω–∞ –º–æ–º–µ–Ω—Ç 18 –¥–µ–∫–∞–±—Ä—è 2025 –≥–æ–¥–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 0.031337‚ÇΩ"
            }
        }

    def _load_database(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        if os.path.exists(self.db_file):
            try:
                with open(self.db_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return self._create_advanced_database()
        return self._create_advanced_database()

    def _create_advanced_database(self) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        db = {
            "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã": {
                "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": {
                    "–Ω–∞—á–∞–ª–æ": [
                        {"word": "–≠—Ç–æ", "weight": 3.0},
                        {"word": "–¢–∞–∫ –Ω–∞–∑—ã–≤–∞—é—Ç", "weight": 2.0},
                        {"word": "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:", "weight": 1.5},
                        {"word": "–í –¥–≤—É—Ö —Å–ª–æ–≤–∞—Ö:", "weight": 1.2}
                    ],
                    "—Å–≤—è–∑–∫–∏": [
                        {"word": "–∫–æ—Ç–æ—Ä–æ–µ", "weight": 2.0},
                        {"word": "–∫–æ—Ç–æ—Ä—ã–π", "weight": 2.0},
                        {"word": "–∫–æ—Ç–æ—Ä–∞—è", "weight": 2.0},
                        {"word": "—è–≤–ª—è–µ—Ç—Å—è", "weight": 1.5},
                        {"word": "–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π", "weight": 1.2}
                    ]
                },
                "–¥–µ–π—Å—Ç–≤–∏–µ": {
                    "–Ω–∞—á–∞–ª–æ": [
                        {"word": "–û–Ω–∞", "weight": 2.0},
                        {"word": "–û–Ω", "weight": 2.0},
                        {"word": "–≠—Ç–æ —Å—É—â–µ—Å—Ç–≤–æ", "weight": 1.5},
                        {"word": "–î–∞–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç", "weight": 1.2}
                    ],
                    "–≥–ª–∞–≥–æ–ª—ã": [
                        {"word": "–¥–µ–ª–∞–µ—Ç", "weight": 2.0},
                        {"word": "—É–º–µ–µ—Ç", "weight": 1.8},
                        {"word": "–º–æ–∂–µ—Ç", "weight": 1.5},
                        {"word": "—Å–ø–æ—Å–æ–±–µ–Ω", "weight": 1.2}
                    ]
                },
                "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ": {
                    "–Ω–∞—á–∞–ª–æ": [
                        {"word": "–ü–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å", "weight": 2.0},
                        {"word": "–í –æ—Ç–ª–∏—á–∏–µ –æ—Ç", "weight": 1.8},
                        {"word": "–¢–∞–∫–∂–µ –∫–∞–∫ –∏", "weight": 1.5},
                        {"word": "–°—Ö–æ–∂ —Å", "weight": 1.2}
                    ],
                    "—Ä–∞–∑–ª–∏—á–∏—è": [
                        {"word": "–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è", "weight": 2.0},
                        {"word": "–∏–º–µ–µ—Ç –æ—Ç–ª–∏—á–∏–µ", "weight": 1.5},
                        {"word": "—Ä–∞–∑–Ω–∏—Ç—Å—è", "weight": 1.2}
                    ]
                }
            },
            "–æ–±—ä–µ–∫—Ç—ã": {
                "–∂–∞–±–∞": {
                    "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": [
                        {"word": "–∑–µ–º–Ω–æ–≤–æ–¥–Ω–æ–µ", "weight": 3.0},
                        {"word": "–∞–º—Ñ–∏–±–∏—è", "weight": 2.0},
                        {"word": "–∂–∏–≤–æ—Ç–Ω–æ–µ", "weight": 1.5}
                    ],
                    "–¥–µ–π—Å—Ç–≤–∏—è": [
                        {"word": "–ø—Ä—ã–≥–∞–µ—Ç", "weight": 2.5},
                        {"word": "–∫–≤–∞–∫–∞–µ—Ç", "weight": 2.0},
                        {"word": "–ø–ª–∞–≤–∞–µ—Ç", "weight": 1.8},
                        {"word": "–ª–æ–≤–∏—Ç –Ω–∞—Å–µ–∫–æ–º—ã—Ö", "weight": 1.5}
                    ],
                    "—Å–≤–æ–π—Å—Ç–≤–∞": [
                        {"word": "–∑–µ–ª–µ–Ω–∞—è", "weight": 2.0},
                        {"word": "–±–æ—Ä–æ–¥–∞–≤—á–∞—Ç–∞—è", "weight": 1.8},
                        {"word": "—Ö–æ–ª–æ–¥–Ω–æ–∫—Ä–æ–≤–Ω–∞—è", "weight": 1.5}
                    ],
                    "–º–µ—Å—Ç–∞": [
                        {"word": "–≤ –ø—Ä—É–¥—É", "weight": 2.0},
                        {"word": "–≤ –±–æ–ª–æ—Ç–µ", "weight": 1.8},
                        {"word": "–≤ —Å–∞–¥—É", "weight": 1.5}
                    ]
                },
                "–ø–∞–π—Ç–æ–Ω": {
                    "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ": [
                        {"word": "—è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "weight": 3.0},
                        {"word": "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "weight": 2.0},
                        {"word": "–ø—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞", "weight": 1.5}
                    ],
                    "–¥–µ–π—Å—Ç–≤–∏—è": [
                        {"word": "–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è", "weight": 2.5},
                        {"word": "–ø–æ–∑–≤–æ–ª—è–µ—Ç", "weight": 2.0},
                        {"word": "–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç", "weight": 1.8}
                    ],
                    "—Å–≤–æ–π—Å—Ç–≤–∞": [
                        {"word": "–ø—Ä–æ—Å—Ç–æ–π", "weight": 2.0},
                        {"word": "–º–Ω–æ–≥–æ—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π", "weight": 1.8},
                        {"word": "–ø–æ–ø—É–ª—è—Ä–Ω—ã–π", "weight": 1.5}
                    ],
                    "–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ": [
                        {"word": "–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏", "weight": 2.0},
                        {"word": "–∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "weight": 1.8},
                        {"word": "–º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è", "weight": 1.5}
                    ]
                }
            },
            "–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã": {
                "–Ω–∞—á–∞–ª–æ": [
                    {"word": "–ï—Å–ª–∏ –≥–æ–≤–æ—Ä–∏—Ç—å –æ", "weight": 2.0},
                    {"word": "–†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—è", "weight": 1.8},
                    {"word": "–ß—Ç–æ –∫–∞—Å–∞–µ—Ç—Å—è", "weight": 1.5},
                    {"word": "–í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ", "weight": 1.2}
                ],
                "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ": [
                    {"word": "–Ω—É–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "weight": 2.0},
                    {"word": "—Å–ª–µ–¥—É–µ—Ç —Å–∫–∞–∑–∞—Ç—å", "weight": 1.8},
                    {"word": "–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å", "weight": 1.5},
                    {"word": "–≤–∞–∂–Ω–æ —É–ø–æ–º—è–Ω—É—Ç—å", "weight": 1.2}
                ],
                "–∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ": [
                    {"word": "–í –∏—Ç–æ–≥–µ", "weight": 2.0},
                    {"word": "–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "weight": 1.8},
                    {"word": "–ò—Ç–∞–∫", "weight": 1.5},
                    {"word": "–í –∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "weight": 1.2}
                ]
            }
        }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        with open(self.db_file, 'w', encoding='utf-8') as f:
            json.dump(db, f, ensure_ascii=False, indent=2)

        return db

    def _analyze_sentence(self, sentence: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ü–µ–ª–∏"""
        sentence_lower = sentence.lower().strip()
        result = {
            "—Ç–∏–ø_–∑–∞–ø—Ä–æ—Å–∞": "–æ–±—â–∞—è_–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
            "–æ–±—ä–µ–∫—Ç": None,
            "–∞—Å–ø–µ–∫—Ç": None,
            "—É—Ç–æ—á–Ω–µ–Ω–∏–µ": None,
            "–∏—Å—Ö–æ–¥–Ω–æ–µ_–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ": sentence
        }

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä–µ–∫—Ç (–≥–ª–∞–≤–Ω–æ–µ —Å–ª–æ–≤–æ)
        words = re.findall(r'\b\w+\b', sentence_lower)

        # –ò—â–µ–º –∑–Ω–∞–∫–æ–º—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        for obj in self.base_knowledge.keys():
            if obj in sentence_lower:
                result["–æ–±—ä–µ–∫—Ç"] = obj
                break

        # –ï—Å–ª–∏ –æ–±—ä–µ–∫—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
        if not result["–æ–±—ä–µ–∫—Ç"] and words:
            result["–æ–±—ä–µ–∫—Ç"] = words[0]

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
        for query_type, patterns in self.context_patterns.items():
            for pattern in patterns:
                if re.search(pattern, sentence_lower):
                    result["—Ç–∏–ø_–∑–∞–ø—Ä–æ—Å–∞"] = query_type

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞—Å–ø–µ–∫—Ç, –µ—Å–ª–∏ –µ—Å—Ç—å
                    if query_type in ["–¥–µ–π—Å—Ç–≤–∏–µ", "—Å–≤–æ–π—Å—Ç–≤–∞"]:
                        # –ò—â–µ–º —á—Ç–æ –∏–º–µ–Ω–Ω–æ —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç
                        action_match = re.search(r"—á—Ç–æ (–¥–µ–ª–∞–µ—Ç|—É–º–µ–µ—Ç)\s+(\w+)", sentence_lower)
                        if action_match:
                            result["–∞—Å–ø–µ–∫—Ç"] = "–¥–µ–π—Å—Ç–≤–∏—è"
                    break

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ (—á—Ç–æ –∏–º–µ–Ω–Ω–æ —Ö–æ—Ç—è—Ç –∑–Ω–∞—Ç—å)
        if "–∫—Ç–æ —Ç–∞–∫–∞—è" in sentence_lower or "—á—Ç–æ —Ç–∞–∫–æ–µ" in sentence_lower:
            result["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] = "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
        elif "—á—Ç–æ –¥–µ–ª–∞–µ—Ç" in sentence_lower or "—á—Ç–æ —É–º–µ–µ—Ç" in sentence_lower:
            result["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] = "–¥–µ–π—Å—Ç–≤–∏—è"
        elif "–≥–¥–µ –∂–∏–≤–µ—Ç" in sentence_lower or "–≥–¥–µ –æ–±–∏—Ç–∞–µ—Ç" in sentence_lower:
            result["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] = "–º–µ—Å—Ç–æ_–æ–±–∏—Ç–∞–Ω–∏—è"
        elif "—á–µ–º –ø–∏—Ç–∞–µ—Ç—Å—è" in sentence_lower or "—á—Ç–æ –µ—Å—Ç" in sentence_lower:
            result["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] = "–ø–∏—Ç–∞–Ω–∏–µ"

        return result

    def _get_synonyms(self, word: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ —Å–ª–æ–≤–∞"""
        return self.synonyms.get(word.lower(), [word])

    def _select_from_list(self, items: List[Dict]) -> str:
        """–í—ã–±–æ—Ä —ç–ª–µ–º–µ–Ω—Ç–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤"""
        if not items:
            return ""

        total_weight = sum(item["weight"] for item in items)
        r = random.uniform(0, total_weight)
        cumulative = 0

        for item in items:
            cumulative += item["weight"]
            if r <= cumulative:
                return item["word"]

        return random.choice([item["word"] for item in items])

    def _generate_definition(self, obj: str, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–∞"""
        chain = []

        # –ù–∞—á–∞–ª–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            start = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"]["–Ω–∞—á–∞–ª–æ"])
            chain.append(start)
        else:
            chain.append("–≠—Ç–æ")

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—ä–µ–∫—Ç
        chain.append(obj)

        # –°–≤—è–∑–∫–∞
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            link = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"]["—Å–≤—è–∑–∫–∏"])
            chain.append(link)

        # –°–∞–º–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–ª–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        if obj in self.base_knowledge:
            definition = self.base_knowledge[obj].get("–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "").split()
            chain.extend(definition)
        elif "–æ–±—ä–µ–∫—Ç—ã" in self.db and obj in self.db["–æ–±—ä–µ–∫—Ç—ã"]:
            if "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" in self.db["–æ–±—ä–µ–∫—Ç—ã"][obj]:
                def_item = self._select_from_list(self.db["–æ–±—ä–µ–∫—Ç—ã"][obj]["–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"])
                chain.append(def_item)

        return chain

    def _generate_actions(self, obj: str, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–µ–π—Å—Ç–≤–∏—è—Ö –æ–±—ä–µ–∫—Ç–∞"""
        chain = []

        # –ù–∞—á–∞–ª–æ –æ–ø–∏—Å–∞–Ω–∏—è –¥–µ–π—Å—Ç–≤–∏–π
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "–¥–µ–π—Å—Ç–≤–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            start = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["–¥–µ–π—Å—Ç–≤–∏–µ"]["–Ω–∞—á–∞–ª–æ"])
            chain.append(start)

        chain.append(obj)

        # –ì–ª–∞–≥–æ–ª
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "–¥–µ–π—Å—Ç–≤–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            verb = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["–¥–µ–π—Å—Ç–≤–∏–µ"]["–≥–ª–∞–≥–æ–ª—ã"])
            chain.append(verb)

        # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
        if obj in self.base_knowledge:
            if "–¥–µ–π—Å—Ç–≤–∏–µ" in self.base_knowledge[obj]:
                actions = self.base_knowledge[obj]["–¥–µ–π—Å—Ç–≤–∏–µ"].split()
                chain.extend(actions)
        elif "–æ–±—ä–µ–∫—Ç—ã" in self.db and obj in self.db["–æ–±—ä–µ–∫—Ç—ã"]:
            if "–¥–µ–π—Å—Ç–≤–∏—è" in self.db["–æ–±—ä–µ–∫—Ç—ã"][obj]:
                action = self._select_from_list(self.db["–æ–±—ä–µ–∫—Ç—ã"][obj]["–¥–µ–π—Å—Ç–≤–∏—è"])
                chain.append(action)

        return chain

    def _generate_comparison(self, obj: str, analysis: Dict) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        chain = []

        # –ù–∞—á–∞–ª–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            start = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ"]["–Ω–∞—á–∞–ª–æ"])
            chain.append(start)

        # –î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω—É–∂–µ–Ω –≤—Ç–æ—Ä–æ–π –æ–±—ä–µ–∫—Ç
        comparable_objs = [o for o in self.db.get("–æ–±—ä–µ–∫—Ç—ã", {}).keys() if o != obj]
        if comparable_objs:
            second_obj = random.choice(comparable_objs)
            chain.append(second_obj)

        chain.append(obj)

        # –£–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Ç–ª–∏—á–∏–µ
        if "–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã" in self.db and "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ" in self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]:
            diff = self._select_from_list(self.db["–∫–æ–Ω—Ç–µ–∫—Å—Ç—ã"]["—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ"]["—Ä–∞–∑–ª–∏—á–∏—è"])
            chain.append(diff)

        # –ß–µ–º –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è
        if obj in self.base_knowledge:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ —Å–≤–æ–π—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–∞
            first_key = next(iter(self.base_knowledge[obj]), None)
            if first_key:
                chain.append(first_key)

        return chain

    def generate_from_sentence(self, sentence: str, max_length: int = 20) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ø–æ—á–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

        Args:
            sentence: –í—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ö—Ç–æ —Ç–∞–∫–∞—è –∂–∞–±–∞?")
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ü–µ–ø–æ—á–∫–∏

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        analysis = self._analyze_sentence(sentence)

        if not analysis["–æ–±—ä–µ–∫—Ç"]:
            return ["–ù–µ", "–º–æ–≥—É", "–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å", "–æ–±—ä–µ–∫—Ç", "–≤", "–∑–∞–ø—Ä–æ—Å–µ"]

        obj = analysis["–æ–±—ä–µ–∫—Ç"]
        query_type = analysis["—Ç–∏–ø_–∑–∞–ø—Ä–æ—Å–∞"]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
        if query_type == "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" or analysis["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] == "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ":
            chain = self._generate_definition(obj, analysis)
        elif query_type == "–¥–µ–π—Å—Ç–≤–∏–µ" or analysis["—É—Ç–æ—á–Ω–µ–Ω–∏–µ"] == "–¥–µ–π—Å—Ç–≤–∏—è":
            chain = self._generate_actions(obj, analysis)
        elif query_type == "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ":
            chain = self._generate_comparison(obj, analysis)
        else:
            # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            chain = []
            if "–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã" in self.db and "–Ω–∞—á–∞–ª–æ" in self.db["–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã"]:
                start = self._select_from_list(self.db["–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã"]["–Ω–∞—á–∞–ª–æ"])
                chain.append(start)

            chain.append(obj)

            if "–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã" in self.db and "–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ" in self.db["–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã"]:
                continuation = self._select_from_list(self.db["–æ–±—â–∏–µ_—Ñ—Ä–∞–∑—ã"]["–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ"])
                chain.append(continuation)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π —Ñ–∞–∫—Ç –æ–± –æ–±—ä–µ–∫—Ç–µ
            if obj in self.base_knowledge:
                random_fact_key = random.choice(list(self.base_knowledge[obj].keys()))
                random_fact = self.base_knowledge[obj][random_fact_key].split()
                chain.extend(random_fact)

        # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è
        if len(chain) > max_length:
            chain = chain[:max_length]

        return chain

    def debug_generation(self, sentence: str, max_length: int = 20) -> List[str]:
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        print(f"\n{'=' * 60}")
        print(f"–ê–ù–ê–õ–ò–ó –ü–†–ï–î–õ–û–ñ–ï–ù–ò–Ø: '{sentence}'")
        print('=' * 60)

        analysis = self._analyze_sentence(sentence)
        print("\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê:")
        for key, value in analysis.items():
            print(f"  {key}: {value}")

        print(f"\nüéØ –û–ë–™–ï–ö–¢: {analysis['–æ–±—ä–µ–∫—Ç']}")
        print(f"üìù –¢–ò–ü –ó–ê–ü–†–û–°–ê: {analysis['—Ç–∏–ø_–∑–∞–ø—Ä–æ—Å–∞']}")

        if analysis['—É—Ç–æ—á–Ω–µ–Ω–∏–µ']:
            print(f"üîç –£–¢–û–ß–ù–ï–ù–ò–ï: {analysis['—É—Ç–æ—á–Ω–µ–Ω–∏–µ']}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
        if analysis['–æ–±—ä–µ–∫—Ç']:
            synonyms = self._get_synonyms(analysis['–æ–±—ä–µ–∫—Ç'])
            print(f"üìö –°–ò–ù–û–ù–ò–ú–´: {', '.join(synonyms)}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –æ–± –æ–±—ä–µ–∫—Ç–µ
        if analysis['–æ–±—ä–µ–∫—Ç'] in self.base_knowledge:
            print(f"\nüí° –ë–ê–ó–û–í–´–ï –ó–ù–ê–ù–ò–Ø –û–ë '{analysis['–æ–±—ä–µ–∫—Ç'].upper()}':")
            for key, value in self.base_knowledge[analysis['–æ–±—ä–µ–∫—Ç']].items():
                print(f"  ‚Ä¢ {key}: {value}")

        print(f"\n{'=' * 60}")
        print("üöÄ –ì–ï–ù–ï–†–ê–¶–ò–Ø –¶–ï–ü–û–ß–ö–ò:")
        print('=' * 60)

        chain = self.generate_from_sentence(sentence, max_length)

        print(f"\nüîó –†–ï–ó–£–õ–¨–¢–ê–¢ ({len(chain)} —Å–ª–æ–≤):")
        print(" ".join(chain))

        return chain

    def add_knowledge(self, obj: str, category: str, knowledge: str, weight: float = 1.0) -> bool:
        """
        –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –æ–± –æ–±—ä–µ–∫—Ç–µ

        Args:
            obj: –û–±—ä–µ–∫—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∂–∞–±–∞")
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–Ω–∞–Ω–∏–π ("–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–¥–µ–π—Å—Ç–≤–∏–µ", –∏ —Ç.–¥.)
            knowledge: –¢–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏—è
            weight: –í–µ—Å –∑–Ω–∞–Ω–∏—è

        Returns:
            bool: –£—Å–ø–µ—à–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ
        """
        try:
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è
            if obj not in self.base_knowledge:
                self.base_knowledge[obj] = {}

            self.base_knowledge[obj][category] = knowledge

            # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ü–µ–ø–æ—á–µ–∫
            if "–æ–±—ä–µ–∫—Ç—ã" not in self.db:
                self.db["–æ–±—ä–µ–∫—Ç—ã"] = {}

            if obj not in self.db["–æ–±—ä–µ–∫—Ç—ã"]:
                self.db["–æ–±—ä–µ–∫—Ç—ã"][obj] = {}

            # –†–∞–∑–±–∏–≤–∞–µ–º –∑–Ω–∞–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
            words = knowledge.split()
            for word in words:
                if category not in self.db["–æ–±—ä–µ–∫—Ç—ã"][obj]:
                    self.db["–æ–±—ä–µ–∫—Ç—ã"][obj][category] = []

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —Ç–∞–∫–æ–µ —Å–ª–æ–≤–æ
                found = False
                for item in self.db["–æ–±—ä–µ–∫—Ç—ã"][obj][category]:
                    if item["word"] == word:
                        item["weight"] += weight
                        found = True
                        break

                if not found:
                    self.db["–æ–±—ä–µ–∫—Ç—ã"][obj][category].append({
                        "word": word,
                        "weight": weight
                    })

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            with open(self.db_file, 'w', encoding='utf-8') as f:
                json.dump(self.db, f, ensure_ascii=False, indent=2)

            return True

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∑–Ω–∞–Ω–∏–π: {e}")
            return False


# –ì–õ–ê–í–ù–´–ï –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø

def generate_context_chain(
        input_text: str,
        chain_length: int = 15,
        db_file: str = "advanced_word_chain_db.json"
) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ü–µ–ø–æ—á–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤–∞ –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)

    Args:
        input_text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç (—Å–ª–æ–≤–æ –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ)
        chain_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ü–µ–ø–æ—á–∫–∏
        db_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ —Ü–µ–ø–æ—á–∫–µ

    –ü—Ä–∏–º–µ—Ä—ã:
        >>> chain = generate_context_chain("–ö—Ç–æ —Ç–∞–∫–∞—è –∂–∞–±–∞?")
        >>> print(" ".join(chain))
        –≠—Ç–æ –∂–∞–±–∞ –∫–æ—Ç–æ—Ä–∞—è –∑–µ–º–Ω–æ–≤–æ–¥–Ω–æ–µ –∂–∏–≤–æ—Ç–Ω–æ–µ

        >>> chain = generate_context_chain("–ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–∞–π—Ç–æ–Ω?")
        >>> print(" ".join(chain))
        –û–Ω –ø–∞–π—Ç–æ–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    """
    generator = AdvancedWordChainGenerator(db_file)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ —Å–ª–æ–≤–æ –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    if len(input_text.split()) <= 2 and not any(punct in input_text for punct in ["?", "!", "."]):
        # –°–ª–æ–≤–æ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –∫–∞–∫ fallback
        return generator.generate_from_sentence(f"—á—Ç–æ —Ç–∞–∫–æ–µ {input_text}", chain_length)
    else:
        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        return generator.generate_from_sentence(input_text, chain_length)


def generate_context_chain_string(
        input_text: str,
        chain_length: int = 15,
        db_file: str = "advanced_word_chain_db.json",
        separator: str = " "
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ü–µ–ø–æ—á–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏

    Args:
        input_text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
        chain_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ü–µ–ø–æ—á–∫–∏
        db_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        separator: –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏

    Returns:
        str: –¶–µ–ø–æ—á–∫–∞ –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
    """
    chain = generate_context_chain(input_text, chain_length, db_file)
    return separator.join(chain)


def add_contextual_knowledge(
        object_name: str,
        question_type: str,
        knowledge_text: str,
        db_file: str = "advanced_word_chain_db.json"
) -> bool:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –æ–± –æ–±—ä–µ–∫—Ç–µ

    Args:
        object_name: –ò–º—è –æ–±—ä–µ–∫—Ç–∞
        question_type: –¢–∏–ø –≤–æ–ø—Ä–æ—Å–∞ ("–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–¥–µ–π—Å—Ç–≤–∏–µ", "—Å–≤–æ–π—Å—Ç–≤–∞", "–ø—Ä–∏–º–µ—Ä—ã")
        knowledge_text: –¢–µ–∫—Å—Ç –∑–Ω–∞–Ω–∏—è
        db_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö

    Returns:
        bool: –£—Å–ø–µ—à–Ω–æ –ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–æ

    –ü—Ä–∏–º–µ—Ä:
        >>> add_contextual_knowledge(
        ...     "–∂–∞–±–∞",
        ...     "–ø–∏—Ç–∞–Ω–∏–µ",
        ...     "–ü–∏—Ç–∞–µ—Ç—Å—è –Ω–∞—Å–µ–∫–æ–º—ã–º–∏, —á–µ—Ä–≤—è–º–∏ –∏ –º–µ–ª–∫–∏–º–∏ –±–µ—Å–ø–æ–∑–≤–æ–Ω–æ—á–Ω—ã–º–∏."
        ... )
        True
    """
    generator = AdvancedWordChainGenerator(db_file)
    return generator.add_knowledge(object_name, question_type, knowledge_text)


# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï
if __name__ == "__main__":
    print("üöÄ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û –ì–ï–ù–ï–†–ê–¢–û–†–ê")
    print("=" * 70)

    generator = AdvancedWordChainGenerator()

    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ö—Ç–æ —Ç–∞–∫–∞—è –∂–∞–±–∞?",
        "–ß—Ç–æ –¥–µ–ª–∞–µ—Ç –∂–∞–±–∞?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø–∞–π—Ç–æ–Ω?",
        "–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Python?",
        "–°—Ä–∞–≤–Ω–∏ –∂–∞–±—É –∏ –ª—è–≥—É—à–∫—É",
        "–ì–¥–µ –∂–∏–≤–µ—Ç –∂–∞–±–∞?",
        "–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –∂–∞–±–∞?",
        "–ß–µ–º –ø–∏—Ç–∞–µ—Ç—Å—è –∂–∞–±–∞?",
        "–ö–∞–∫–∏–µ —Å–≤–æ–π—Å—Ç–≤–∞ —É –ø–∞–π—Ç–æ–Ω–∞?",
        "–û–±—ä—è—Å–Ω–∏ –ø—Ä–æ –ì–∞–≤—Ä–∏–ª–æ–≤–Ω—É",
        "–ß—Ç–æ –∑–∞ –∫—É—Ä—Å –±—É–±–ª–∏–∫–∞?"
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\n{'=' * 70}")
        print(f"–¢–ï–°–¢ {i}: {query}")
        print('=' * 70)

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        chain = generator.debug_generation(query, 15)

        # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—ã—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"\nüìã –ö–û–ú–ü–ê–ö–¢–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: {' '.join(chain)}")

    print(f"\n{'=' * 70}")
    print("–î–û–ë–ê–í–õ–ï–ù–ò–ï –ù–û–í–´–• –ó–ù–ê–ù–ò–ô")
    print('=' * 70)

    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è
    print("\n‚ûï –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è –æ —Å–æ–±–∞–∫–µ...")
    add_contextual_knowledge(
        "—Å–æ–±–∞–∫–∞",
        "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
        "–°–æ–±–∞–∫–∞ ‚Äî —ç—Ç–æ –¥–æ–º–∞—à–Ω–µ–µ –∂–∏–≤–æ—Ç–Ω–æ–µ, –¥—Ä—É–≥ —á–µ–ª–æ–≤–µ–∫–∞."
    )
    add_contextual_knowledge(
        "—Å–æ–±–∞–∫–∞",
        "–¥–µ–π—Å—Ç–≤–∏–µ",
        "–°–æ–±–∞–∫–∞ –ª–∞–µ—Ç, –±–µ–≥–∞–µ—Ç, –∏–≥—Ä–∞–µ—Ç –∏ –æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–º."
    )
    add_contextual_knowledge(
        "—Å–æ–±–∞–∫–∞",
        "–ø–∏—Ç–∞–Ω–∏–µ",
        "–ü–∏—Ç–∞–µ—Ç—Å—è –º—è—Å–æ–º, –∫–æ—Ä–º–æ–º –∏ –∫–æ—Å—Ç—è–º–∏."
    )

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –Ω–æ–≤—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏
    print("\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å –Ω–æ–≤—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏...")
    chain = generate_context_chain("–ß—Ç–æ –¥–µ–ª–∞–µ—Ç —Å–æ–±–∞–∫–∞?")
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {' '.join(chain)}")

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print(f"\n{'=' * 70}")
    print("–ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø")
    print('=' * 70)

    examples = [
        ("–ö—Ç–æ —Ç–∞–∫–∞—è –∂–∞–±–∞?", "–û–¥–∏–Ω –≤–æ–ø—Ä–æ—Å"),
        ("–ø–∞–π—Ç–æ–Ω", "–û–¥–Ω–æ —Å–ª–æ–≤–æ"),
        ("–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç –∂–∞–±–∞ –∏ –≥–¥–µ –æ–Ω–∞ –∂–∏–≤–µ—Ç?", "–°–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å")
    ]

    for query, description in examples:
        result = generate_context_chain_string(query, separator=" ‚Üí ")
        print(f"\nüìù {description}: '{query}'")
        print(f"üîó –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")

    print(f"\n{'=' * 70}")
    print("‚úÖ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
    print("=" * 70)